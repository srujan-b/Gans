{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2756448-1140-4c69-9eea-f768ddad0b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 22:13:29.273302: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-05 22:13:30.156396: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-05 22:13:30.373176: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-05 22:13:31.841390: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-05 22:13:41.228576: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input,Dense,Reshape,Flatten,BatchNormalization,LeakyReLU\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import os \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f6010b-0a93-4d14-a2b8-ab70c0750fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining image dimensions\n",
    "\n",
    "imgRows = 28\n",
    "imgCols = 28\n",
    "channels = 1\n",
    "imgShape = (imgRows,imgCols,channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b276af3-3e85-4432-921a-85eedb0a2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "\n",
    "    \"\"\"\n",
    "        Creates a generator model for a Generative Adversaial Network (GAN).\n",
    "\n",
    "        The generator takes a noise vector as input and transforms it into realistic looking image through a series of fully connected layers,Leaky Relu \n",
    "        activation , batch normalizations and reshaping. The final output is scaled tot the range [-1,1] using the 'tanh' activation function,\n",
    "        making it suitable for image generation tasks.\n",
    "    \n",
    "        return:\n",
    "            keras.Model : A keras model that maps noise vectors to generated images.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #Define the shape of the noise vector ; this will serve as the input to the generastor \n",
    "    # typically used in GANS, the noise vector allows the model to generate diverse outputs\n",
    "    noise_shape = (100,)\n",
    "\n",
    "    # Initilize a sequential model , which is a linear stack of layers\n",
    "    model = Sequential()\n",
    "\n",
    "    #Add a dense layer with 256 neurons, this layer acts as the first fully connected layer, transforming the \n",
    "    #input noise vector into a higher dimensional feature space.\n",
    "    #the input_shape defines the expected input noise shape dimensions \n",
    "    model.add(Dense(256,input_shape = noise_shape))\n",
    "    \n",
    "    # Add a LekyRelu activation function with a small negative slope defined by 'alpha'\n",
    "    # LeakyRelu is a variant of the standard Relu (Rectified linear unit) actibvtion function\n",
    "    # While standard RELU sets the value to 0 for all the negative inputs , this allows a small, non zero gradient for negative inputs\n",
    "    # This helps mitigate the \"dying RELU\" problem, where neurons become inactive and stop learning due to 0 gradient\n",
    "    # the alpha parameter defines the slope of the activatioin function for negative inputs. A smaller alpha means means less contibution from negative values \n",
    "    # large alpha means it allows more contribution from neagtive values.\n",
    "    # this activation helps prevent the 'dying relu' problem by allowing small gradient for negative inputs\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "\n",
    "    # Add batch normalization layer to stablize and accelarate training by normalizing the activations of the previous layer.\n",
    "    # The momentum parameter controls how much of the past running statics to use.\n",
    "    # This layer also prevents internal covariate shift and improves the models generalization ability\n",
    "    model.add(BatchNormalization(momentum = 0.8))\n",
    "\n",
    "    # Add a Dense layer with 512 neurons to further expand the feature space.\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    model.add(BatchNormalization(momentum = 0.8))\n",
    "\n",
    "    # Add another Dense layer with 1024 neurons for further expansion.\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    model.add(BatchNormalization(momentum = 0.8))\n",
    "\n",
    "    # Add the output Dense Layer to produce the final generated image.\n",
    "    # np.prod(image shape) caluclates the total number of pixels (flattned shape) of the output image.\n",
    "    # the activation function 'tanh' ensures that the output values are scaled bwteen -1 and 1 which is common for image generation tasks \n",
    "    model.add(Dense(np.prod(imgShape),activation = 'tanh'))\n",
    "\n",
    "    # Reshape the output to match the desired image dimensions(img Shape).\n",
    "    model.add(Reshape(imgShape))\n",
    "\n",
    "    #print model summary of the model architecture.\n",
    "    #model.summary()\n",
    "\n",
    "    # define the input to the generastor which is the noise vector\n",
    "    noise = Input(shape = noise_shape)\n",
    "\n",
    "    # pass the noise vector through the model to generate image \n",
    "    img = model(noise)\n",
    "\n",
    "    # return the generator model which maps noise vector to generated images\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fabb137f-8f7f-4682-8a14-5dbebc67b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    \"\"\"\n",
    "    Builds a desciminator model for Generative adversial network (GAN)\n",
    "\n",
    "    The descriminator acts as a binary classifier that distuingishes between real and fake images.\n",
    "    It takes an image as input ,flatten it into vector, and processes it through series of fully connected layers with leakyRelu activations. \n",
    "    The final layer uses sigmoid activation function output the porbality value representing the validity of the input image.\n",
    "\n",
    "    Returns:\n",
    "    Keras.Model : A keras model that maps an input image to validity score(0  to 1)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # initilize a sequential model for descriminators\n",
    "    model = Sequential()\n",
    "\n",
    "    # Flatten the input image from its original shape in 1D vector\n",
    "    # this prepares the image for fully connected layers\n",
    "    model.add(Flatten(input_shape = imgShape))\n",
    "\n",
    "    #Add a dense layer with 512 neurons to process the flatten layer\n",
    "    # this layer helps in learning higher-level features from input\n",
    "    model.add(Dense(512))\n",
    "\n",
    "    # add leaky relu activation function to introduce non-linearity.\n",
    "    #the small negative slope (alpha = 0.2) ensures small gradient for negative inputs.\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    #add another Dense Layer with 256 neurons for further feature extraction\n",
    "    model.add(Dense(256))\n",
    "\n",
    "    #add another LeakyRelu activation for non - linearity.\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # add the output Dense layer with 1 neuron and a single sigmoid activation function.\n",
    "    #Thje sigmoid function outputs probality between 0 and 1 , representing wheather the input image is real (closer to 1 ) or fake (closer to 0).\n",
    "    \n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "    # prints the model summary\n",
    "    #model.summary()\n",
    "\n",
    "    # defines the input to the descriminator, which is an image with same shape as the generator shape\n",
    "    img = Input(shape = imgShape)\n",
    "\n",
    "    #pass the input through the model to get the validity score.\n",
    "    validity = model(img)\n",
    "\n",
    "    #return the descriminator model, which maps an input to a validity score.\n",
    "    return Model(img,validity)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f5b95-eab8-4288-baca-7626b10b4f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736095588.616024  244811 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1736095590.936889  244811 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1736095590.936952  244811 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1736095590.943164  244811 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1736095590.943230  244811 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1736095590.943258  244811 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1736095591.130715  244811 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1736095591.130906  244811 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-05 22:16:31.130932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1736095591.131075  244811 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-05 22:16:31.138431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "/mnt/c/Users/somes/gans/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/mnt/c/Users/somes/gans/lib/python3.10/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/somes/gans/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/somes/gans/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:75: UserWarning: The model does not have any trainable weights.\n",
      "  warnings.warn(\"The model does not have any trainable weights.\")\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736095599.466993  245024 service.cc:146] XLA service 0x7f3a7c0061c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1736095599.467052  245024 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-01-05 22:16:39.625923: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-01-05 22:16:39.838066: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "I0000 00:00:1736095600.325525  245024 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-01-05 22:16:42.363780: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_506', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-01-05 22:16:42.842927: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_506', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-01-05 22:16:43.518030: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_506', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-01-05 22:16:43.650886: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_506', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-01-05 22:16:43.897468: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_506', 252 bytes spill stores, 252 bytes spill loads\n",
      "\n",
      "2025-01-05 22:16:43.948279: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_506', 468 bytes spill stores, 468 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.825148, acc.: 30.47%] [G loss: 0.722075]\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7f3b32d85ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7f3b32d85ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7f3b32d87d00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7f3b32d87d00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.858157, acc.: 31.18%] [G loss: 0.728026]\n",
      "[D loss: 0.874573, acc.: 28.67%] [G loss: 0.737401]\n",
      "[D loss: 0.894870, acc.: 26.45%] [G loss: 0.745226]\n",
      "[D loss: 0.913883, acc.: 23.41%] [G loss: 0.749728]\n",
      "[D loss: 0.929152, acc.: 22.39%] [G loss: 0.759749]\n",
      "[D loss: 0.946834, acc.: 21.03%] [G loss: 0.770419]\n",
      "[D loss: 0.960610, acc.: 20.38%] [G loss: 0.778509]\n",
      "[D loss: 0.976909, acc.: 19.48%] [G loss: 0.789917]\n",
      "[D loss: 0.994013, acc.: 18.60%] [G loss: 0.798793]\n",
      "[D loss: 1.007830, acc.: 18.11%] [G loss: 0.809620]\n",
      "[D loss: 1.023092, acc.: 17.83%] [G loss: 0.819732]\n",
      "[D loss: 1.041345, acc.: 17.22%] [G loss: 0.833934]\n",
      "[D loss: 1.059220, acc.: 16.88%] [G loss: 0.844981]\n",
      "[D loss: 1.075599, acc.: 16.64%] [G loss: 0.857052]\n",
      "[D loss: 1.091613, acc.: 16.13%] [G loss: 0.867421]\n",
      "[D loss: 1.106959, acc.: 15.91%] [G loss: 0.877828]\n",
      "[D loss: 1.121282, acc.: 15.72%] [G loss: 0.888327]\n",
      "[D loss: 1.136640, acc.: 15.79%] [G loss: 0.900710]\n",
      "[D loss: 1.152727, acc.: 15.86%] [G loss: 0.912679]\n",
      "[D loss: 1.166848, acc.: 15.82%] [G loss: 0.922479]\n",
      "[D loss: 1.180968, acc.: 15.63%] [G loss: 0.932949]\n",
      "[D loss: 1.195664, acc.: 15.39%] [G loss: 0.943492]\n",
      "[D loss: 1.210432, acc.: 15.20%] [G loss: 0.953787]\n",
      "[D loss: 1.223357, acc.: 15.22%] [G loss: 0.963230]\n",
      "[D loss: 1.236230, acc.: 15.17%] [G loss: 0.972965]\n",
      "[D loss: 1.250345, acc.: 15.13%] [G loss: 0.983565]\n",
      "[D loss: 1.264987, acc.: 15.12%] [G loss: 0.995052]\n",
      "[D loss: 1.279148, acc.: 15.11%] [G loss: 1.004771]\n",
      "[D loss: 1.292754, acc.: 15.05%] [G loss: 1.014814]\n",
      "[D loss: 1.305285, acc.: 15.22%] [G loss: 1.024789]\n",
      "[D loss: 1.319328, acc.: 15.13%] [G loss: 1.034935]\n",
      "[D loss: 1.331896, acc.: 15.24%] [G loss: 1.044567]\n",
      "[D loss: 1.345288, acc.: 15.14%] [G loss: 1.054193]\n",
      "[D loss: 1.359093, acc.: 14.97%] [G loss: 1.064511]\n",
      "[D loss: 1.372128, acc.: 14.97%] [G loss: 1.073711]\n",
      "[D loss: 1.383992, acc.: 15.03%] [G loss: 1.082601]\n",
      "[D loss: 1.395463, acc.: 15.13%] [G loss: 1.092035]\n",
      "[D loss: 1.408135, acc.: 15.00%] [G loss: 1.100423]\n",
      "[D loss: 1.419235, acc.: 15.06%] [G loss: 1.109082]\n",
      "[D loss: 1.430860, acc.: 15.03%] [G loss: 1.117545]\n",
      "[D loss: 1.443180, acc.: 14.91%] [G loss: 1.126909]\n",
      "[D loss: 1.455530, acc.: 14.93%] [G loss: 1.136173]\n",
      "[D loss: 1.467567, acc.: 14.96%] [G loss: 1.145204]\n",
      "[D loss: 1.479842, acc.: 14.89%] [G loss: 1.154098]\n",
      "[D loss: 1.491264, acc.: 14.91%] [G loss: 1.162550]\n",
      "[D loss: 1.502747, acc.: 14.96%] [G loss: 1.171188]\n",
      "[D loss: 1.514674, acc.: 14.81%] [G loss: 1.179746]\n",
      "[D loss: 1.525100, acc.: 14.79%] [G loss: 1.187237]\n",
      "[D loss: 1.535856, acc.: 14.73%] [G loss: 1.195052]\n",
      "[D loss: 1.546632, acc.: 14.70%] [G loss: 1.203490]\n",
      "[D loss: 1.556725, acc.: 14.76%] [G loss: 1.210662]\n",
      "[D loss: 1.566725, acc.: 14.72%] [G loss: 1.218210]\n",
      "[D loss: 1.576639, acc.: 14.70%] [G loss: 1.225344]\n",
      "[D loss: 1.586440, acc.: 14.71%] [G loss: 1.232703]\n",
      "[D loss: 1.596724, acc.: 14.66%] [G loss: 1.240427]\n",
      "[D loss: 1.606503, acc.: 14.66%] [G loss: 1.247833]\n",
      "[D loss: 1.616733, acc.: 14.62%] [G loss: 1.255329]\n",
      "[D loss: 1.626625, acc.: 14.53%] [G loss: 1.262264]\n",
      "[D loss: 1.635931, acc.: 14.53%] [G loss: 1.269035]\n",
      "[D loss: 1.645135, acc.: 14.56%] [G loss: 1.275982]\n",
      "[D loss: 1.653620, acc.: 14.60%] [G loss: 1.282294]\n",
      "[D loss: 1.662439, acc.: 14.58%] [G loss: 1.289086]\n",
      "[D loss: 1.671389, acc.: 14.58%] [G loss: 1.295547]\n",
      "[D loss: 1.680108, acc.: 14.59%] [G loss: 1.302288]\n",
      "[D loss: 1.689144, acc.: 14.56%] [G loss: 1.308838]\n",
      "[D loss: 1.698176, acc.: 14.58%] [G loss: 1.315759]\n",
      "[D loss: 1.707312, acc.: 14.53%] [G loss: 1.322330]\n",
      "[D loss: 1.716357, acc.: 14.45%] [G loss: 1.329139]\n",
      "[D loss: 1.725106, acc.: 14.46%] [G loss: 1.335556]\n",
      "[D loss: 1.733050, acc.: 14.53%] [G loss: 1.341222]\n",
      "[D loss: 1.741327, acc.: 14.43%] [G loss: 1.347664]\n",
      "[D loss: 1.749823, acc.: 14.52%] [G loss: 1.354072]\n",
      "[D loss: 1.758305, acc.: 14.54%] [G loss: 1.360370]\n",
      "[D loss: 1.766290, acc.: 14.54%] [G loss: 1.366326]\n",
      "[D loss: 1.774340, acc.: 14.61%] [G loss: 1.372824]\n",
      "[D loss: 1.782829, acc.: 14.56%] [G loss: 1.378654]\n",
      "[D loss: 1.790508, acc.: 14.56%] [G loss: 1.384477]\n",
      "[D loss: 1.798540, acc.: 14.58%] [G loss: 1.390683]\n",
      "[D loss: 1.806636, acc.: 14.60%] [G loss: 1.396709]\n",
      "[D loss: 1.814922, acc.: 14.59%] [G loss: 1.402941]\n",
      "[D loss: 1.822687, acc.: 14.55%] [G loss: 1.408282]\n",
      "[D loss: 1.830110, acc.: 14.55%] [G loss: 1.414101]\n",
      "[D loss: 1.838292, acc.: 14.53%] [G loss: 1.420338]\n",
      "[D loss: 1.845832, acc.: 14.56%] [G loss: 1.425676]\n",
      "[D loss: 1.853223, acc.: 14.56%] [G loss: 1.431232]\n",
      "[D loss: 1.860887, acc.: 14.56%] [G loss: 1.437141]\n",
      "[D loss: 1.868700, acc.: 14.54%] [G loss: 1.442948]\n",
      "[D loss: 1.876438, acc.: 14.53%] [G loss: 1.448580]\n",
      "[D loss: 1.883446, acc.: 14.49%] [G loss: 1.453345]\n",
      "[D loss: 1.890128, acc.: 14.51%] [G loss: 1.458715]\n",
      "[D loss: 1.897370, acc.: 14.48%] [G loss: 1.463935]\n",
      "[D loss: 1.904415, acc.: 14.50%] [G loss: 1.469332]\n",
      "[D loss: 1.911500, acc.: 14.43%] [G loss: 1.474278]\n",
      "[D loss: 1.918364, acc.: 14.46%] [G loss: 1.479725]\n",
      "[D loss: 1.925166, acc.: 14.47%] [G loss: 1.484541]\n",
      "[D loss: 1.931900, acc.: 14.45%] [G loss: 1.489607]\n",
      "[D loss: 1.938145, acc.: 14.47%] [G loss: 1.494163]\n",
      "[D loss: 1.944346, acc.: 14.45%] [G loss: 1.498711]\n",
      "[D loss: 1.950402, acc.: 14.43%] [G loss: 1.503081]\n",
      "[D loss: 1.956674, acc.: 14.39%] [G loss: 1.507888]\n",
      "[D loss: 1.962856, acc.: 14.40%] [G loss: 1.512517]\n",
      "[D loss: 1.968830, acc.: 14.42%] [G loss: 1.516668]\n",
      "[D loss: 1.974671, acc.: 14.42%] [G loss: 1.521413]\n",
      "[D loss: 1.980950, acc.: 14.45%] [G loss: 1.526206]\n",
      "[D loss: 1.987411, acc.: 14.44%] [G loss: 1.530937]\n",
      "[D loss: 1.993509, acc.: 14.43%] [G loss: 1.535547]\n",
      "[D loss: 1.999739, acc.: 14.44%] [G loss: 1.540246]\n",
      "[D loss: 2.006183, acc.: 14.42%] [G loss: 1.544935]\n",
      "[D loss: 2.012134, acc.: 14.42%] [G loss: 1.549180]\n",
      "[D loss: 2.017557, acc.: 14.45%] [G loss: 1.553360]\n",
      "[D loss: 2.023647, acc.: 14.37%] [G loss: 1.557887]\n",
      "[D loss: 2.029981, acc.: 14.32%] [G loss: 1.562550]\n",
      "[D loss: 2.035919, acc.: 14.33%] [G loss: 1.566809]\n",
      "[D loss: 2.041871, acc.: 14.30%] [G loss: 1.571271]\n",
      "[D loss: 2.047545, acc.: 14.29%] [G loss: 1.575309]\n",
      "[D loss: 2.053137, acc.: 14.30%] [G loss: 1.579706]\n",
      "[D loss: 2.058886, acc.: 14.34%] [G loss: 1.584200]\n",
      "[D loss: 2.065052, acc.: 14.29%] [G loss: 1.588604]\n",
      "[D loss: 2.070463, acc.: 14.31%] [G loss: 1.592453]\n",
      "[D loss: 2.075876, acc.: 14.33%] [G loss: 1.596701]\n",
      "[D loss: 2.081750, acc.: 14.32%] [G loss: 1.601128]\n",
      "[D loss: 2.087537, acc.: 14.32%] [G loss: 1.605382]\n",
      "[D loss: 2.092897, acc.: 14.33%] [G loss: 1.609338]\n",
      "[D loss: 2.098095, acc.: 14.31%] [G loss: 1.613077]\n",
      "[D loss: 2.103377, acc.: 14.31%] [G loss: 1.617306]\n",
      "[D loss: 2.108831, acc.: 14.31%] [G loss: 1.621198]\n",
      "[D loss: 2.114273, acc.: 14.31%] [G loss: 1.625391]\n",
      "[D loss: 2.119793, acc.: 14.30%] [G loss: 1.629397]\n",
      "[D loss: 2.124824, acc.: 14.28%] [G loss: 1.632938]\n",
      "[D loss: 2.129606, acc.: 14.32%] [G loss: 1.636708]\n",
      "[D loss: 2.134690, acc.: 14.31%] [G loss: 1.640493]\n",
      "[D loss: 2.139512, acc.: 14.31%] [G loss: 1.644029]\n",
      "[D loss: 2.144750, acc.: 14.33%] [G loss: 1.648261]\n",
      "[D loss: 2.150235, acc.: 14.26%] [G loss: 1.652004]\n",
      "[D loss: 2.155188, acc.: 14.28%] [G loss: 1.655743]\n",
      "[D loss: 2.160227, acc.: 14.29%] [G loss: 1.659696]\n",
      "[D loss: 2.165406, acc.: 14.27%] [G loss: 1.663381]\n",
      "[D loss: 2.170480, acc.: 14.27%] [G loss: 1.667174]\n",
      "[D loss: 2.175341, acc.: 14.30%] [G loss: 1.670828]\n",
      "[D loss: 2.180326, acc.: 14.33%] [G loss: 1.674750]\n",
      "[D loss: 2.185188, acc.: 14.32%] [G loss: 1.678103]\n",
      "[D loss: 2.190084, acc.: 14.32%] [G loss: 1.682097]\n",
      "[D loss: 2.195074, acc.: 14.30%] [G loss: 1.685516]\n",
      "[D loss: 2.199858, acc.: 14.35%] [G loss: 1.689463]\n",
      "[D loss: 2.205124, acc.: 14.38%] [G loss: 1.693506]\n",
      "[D loss: 2.210177, acc.: 14.35%] [G loss: 1.697002]\n",
      "[D loss: 2.214949, acc.: 14.34%] [G loss: 1.700542]\n",
      "[D loss: 2.219453, acc.: 14.34%] [G loss: 1.703733]\n",
      "[D loss: 2.224019, acc.: 14.33%] [G loss: 1.707286]\n",
      "[D loss: 2.228737, acc.: 14.30%] [G loss: 1.710752]\n",
      "[D loss: 2.233304, acc.: 14.30%] [G loss: 1.714011]\n",
      "[D loss: 2.237500, acc.: 14.29%] [G loss: 1.717157]\n",
      "[D loss: 2.241991, acc.: 14.28%] [G loss: 1.720580]\n",
      "[D loss: 2.246534, acc.: 14.29%] [G loss: 1.724042]\n",
      "[D loss: 2.250864, acc.: 14.26%] [G loss: 1.726945]\n",
      "[D loss: 2.255157, acc.: 14.25%] [G loss: 1.730343]\n",
      "[D loss: 2.259491, acc.: 14.24%] [G loss: 1.733427]\n",
      "[D loss: 2.264078, acc.: 14.19%] [G loss: 1.736918]\n",
      "[D loss: 2.268634, acc.: 14.18%] [G loss: 1.740182]\n",
      "[D loss: 2.273103, acc.: 14.16%] [G loss: 1.743515]\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'images/' folder exists\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# Train Function\n",
    "def train(epochs, batchSize=128, saveInterval=50):\n",
    "    # Load the MNIST dataset\n",
    "    ds_train, ds_info = tfds.load(\n",
    "        'mnist', split='train', shuffle_files=True, as_supervised=True, with_info=True\n",
    "    )\n",
    "    ds_train_images = ds_train.map(lambda image, label: image)\n",
    "\n",
    "    def normalize_img(image):\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1.0\n",
    "        image = tf.expand_dims(image, axis=-1)\n",
    "        return image\n",
    "\n",
    "    ds_train_images = ds_train_images.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds_train_images = ds_train_images.cache()\n",
    "    ds_train_images = ds_train_images.batch(batchSize)\n",
    "    ds_train_images = ds_train_images.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    halfBatch = int(batchSize / 2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        \n",
    "        for real_imgs in ds_train_images:\n",
    "            # Train Discriminator\n",
    "            idx = np.random.randint(0, real_imgs.shape[0], halfBatch)\n",
    "            real_half_batch = tf.gather(real_imgs, idx)\n",
    "    \n",
    "            noise = np.random.normal(0, 1, (halfBatch, 100))\n",
    "            gen_imgs = generator_model(noise, training=True)\n",
    "    \n",
    "            \n",
    "            # Ensure the shape of real_half_batch is correct\n",
    "            if real_half_batch.shape[-1] == 1:\n",
    "                real_half_batch = tf.squeeze(real_half_batch, axis=-1)\n",
    "    \n",
    "            d_loss_real = discriminator_model.train_on_batch(real_half_batch, tf.ones((halfBatch, 1)))\n",
    "            d_loss_fake = discriminator_model.train_on_batch(gen_imgs, tf.zeros((halfBatch, 1)))\n",
    "    \n",
    "            d_loss = 0.5 * tf.add(d_loss_real, d_loss_fake).numpy()  # Convert to NumPy\n",
    "    \n",
    "            # Train Generator\n",
    "            noise = np.random.normal(0, 1, (batchSize, 100))\n",
    "            valid_y = np.ones((batchSize, 1))\n",
    "            g_loss = combined.train_on_batch(noise, valid_y)\n",
    "    \n",
    "            print(\"[D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (d_loss[0], 100 * d_loss[1], sum(g_loss)/ len(g_loss)))\n",
    "    \n",
    "        if epoch % saveInterval == 0:\n",
    "            save_imgs(epoch)\n",
    "\n",
    "\n",
    "# Save Images\n",
    "def save_imgs(epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator_model.predict(noise)\n",
    "\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(f\"/images/mnist_{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Discriminator\n",
    "discriminator_model = discriminator()\n",
    "discriminator_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Generator\n",
    "generator_model = generator()\n",
    "generator_model.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "# Combined Model\n",
    "z = Input(shape=(100,))\n",
    "img = generator_model(z)\n",
    "discriminator_model.trainable = False\n",
    "valid = discriminator_model(img)\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "# Train GAN\n",
    "train(epochs=1, batchSize=256, saveInterval=10)\n",
    "\n",
    "# Save Generator Model\n",
    "generator_model.save('generator_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186519d5-d112-4229-9761-9837aab67fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
