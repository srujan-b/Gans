{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2756448-1140-4c69-9eea-f768ddad0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Dense,Reshape,Flatten,BatchNormalization,LeakyReLU\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8a5377-e1f9-438a-8b26-885e08e41f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining image dimensions\n",
    "\n",
    "imgRows = 28\n",
    "imgCols = 28\n",
    "channels = 1\n",
    "imgShape = (imgRows,imgCols,channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b276af3-3e85-4432-921a-85eedb0a2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "\n",
    "    \"\"\"\n",
    "        Creates a generator model for a Generative Adversaial Network (GAN).\n",
    "\n",
    "        The generator takes a noise vector as input and transforms it into realistic looking image through a series of fully connected layers,Leaky Relu \n",
    "        activation , batch normalizations and reshaping. The final output is scaled tot the range [-1,1] using the 'tanh' activation function,\n",
    "        making it suitable for image generation tasks.\n",
    "    \n",
    "        return:\n",
    "            keras.Model : A keras model that maps noise vectors to generated images.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #Define the shape of the noise vector ; this will serve as the input to the generastor \n",
    "    # typically used in GANS, the noise vector allows the model to generate diverse outputs\n",
    "    noise_shape = (100,)\n",
    "\n",
    "    # Initilize a sequential model , which is a linear stack of layers\n",
    "    model = Sequential()\n",
    "\n",
    "    #Add a dense layer with 256 neurons, this layer acts as the first fully connected layer, transforming the \n",
    "    #input noise vector into a higher dimensional feature space.\n",
    "    #the input_shape defines the expected input noise shape dimensions \n",
    "    model.add(Dense(256,input_shape = noise_shape))\n",
    "    \n",
    "    # Add a LekyRelu activation function with a small negative slope defined by 'alpha'\n",
    "    # LeakyRelu is a variant of the standard Relu (Rectified linear unit) actibvtion function\n",
    "    # While standard RELU sets the value to 0 for all the negative inputs , this allows a small, non zero gradient for negative inputs\n",
    "    # This helps mitigate the \"dying RELU\" problem, where neurons become inactive and stop learning due to 0 gradient\n",
    "    # the alpha parameter defines the slope of the activatioin function for negative inputs. A smaller alpha means means less contibution from negative values \n",
    "    # large alpha means it allows more contribution from neagtive values.\n",
    "    # this activation helps prevent the 'dying relu' problem by allowing small gradient for negative inputs\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "\n",
    "    # Add batch normalization layer to stablize and accelarate training by normalizing the activations of the previous layer.\n",
    "    # The momentum parameter controls how much of the past running statics to use.\n",
    "    # This layer also prevents internal covariate shift and improves the models generalization ability\n",
    "    model.add(BatchNormalization(momentum = 0.8))\n",
    "\n",
    "    # Add a Dense layer with 512 neurons to further expand the feature space.\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    model.add(BatchNormalization(momentum = 0.8))\n",
    "\n",
    "    # Add another Dense layer with 1024 neurons for further expansion.\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    model.add(BatchNormalization(momentum = 0.8))\n",
    "\n",
    "    # Add the output Dense Layer to produce the final generated image.\n",
    "    # np.prod(image shape) caluclates the total number of pixels (flattned shape) of the output image.\n",
    "    # the activation function 'tanh' ensures that the output values are scaled bwteen -1 and 1 which is common for image generation tasks \n",
    "    model.add(Dense(np.prod(imgShape),activation = 'tanh'))\n",
    "\n",
    "    # Reshape the output to match the desired image dimensions(img Shape).\n",
    "    model.add(Reshape(imgShape))\n",
    "\n",
    "    #print model summary of the model architecture.\n",
    "    model.summary()\n",
    "\n",
    "    # define the input to the generastor which is the noise vector\n",
    "    noise = Input(shape = noise_shape)\n",
    "\n",
    "    # pass the noise vector through the model to generate image \n",
    "    img = model(noise)\n",
    "\n",
    "    # return the generator model which maps noise vector to generated images\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fabb137f-8f7f-4682-8a14-5dbebc67b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    \"\"\"\n",
    "    Builds a desciminator model for Generative adversial network (GAN)\n",
    "\n",
    "    The descriminator acts as a binary classifier that distuingishes between real and fake images.\n",
    "    It takes an image as input ,flatten it into vector, and processes it through series of fully connected layers with leakyRelu activations. \n",
    "    The final layer uses sigmoid activation function output the porbality value representing the validity of the input image.\n",
    "\n",
    "    Returns:\n",
    "    Keras.Model : A keras model that maps an input image to validity score(0  to 1)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # initilize a sequential model for descriminators\n",
    "    model = Sequential()\n",
    "\n",
    "    # Flatten the input image from its original shape in 1D vector\n",
    "    # this prepares the image for fully connected layers\n",
    "    model.add(Flatten(input_shape = imgShape))\n",
    "\n",
    "    #Add a dense layer with 512 neurons to process the flatten layer\n",
    "    # this layer helps in learning higher-level features from input\n",
    "    model.add(Dense(512))\n",
    "\n",
    "    # add leaky relu activation function to introduce non-linearity.\n",
    "    #the small negative slope (alpha = 0.2) ensures small gradient for negative inputs.\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    #add another Dense Layer with 256 neurons for further feature extraction\n",
    "    model.add(Dense(256))\n",
    "\n",
    "    #add another LeakyRelu activation for non - linearity.\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # add the output Dense layer with 1 neuron and a single sigmoid activation function.\n",
    "    #Thje sigmoid function outputs probality between 0 and 1 , representing wheather the input image is real (closer to 1 ) or fake (closer to 0).\n",
    "    \n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "    # prints the model summary\n",
    "    model.summary()\n",
    "\n",
    "    # defines the input to the descriminator, which is an image with same shape as the generator shape\n",
    "    img = Input(shape = imgShape)\n",
    "\n",
    "    #pass the input through the model to get the validity score.\n",
    "    validity = model(img)\n",
    "\n",
    "    #return the descriminator model, which maps an input to a validity score.\n",
    "    return Model(img,validity)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a81f5b95-eab8-4288-baca-7626b10b4f69",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_207111/2540646253.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Ensure 'images/' folder exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaveInterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Ensure 'images/' folder exists\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# Train Function\n",
    "def train(epochs, batchSize=128, saveInterval=50):\n",
    "    # Load the MNIST dataset\n",
    "    ds_train, ds_info = tfds.load(\n",
    "        'mnist', split='train', shuffle_files=True, as_supervised=True, with_info=True\n",
    "    )\n",
    "    ds_train_images = ds_train.map(lambda image, label: image)\n",
    "\n",
    "    def normalize_img(image):\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1.0\n",
    "        image = tf.expand_dims(image, axis=-1)\n",
    "        return image\n",
    "\n",
    "    ds_train_images = ds_train_images.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds_train_images = ds_train_images.cache()\n",
    "    ds_train_images = ds_train_images.batch(batchSize)\n",
    "    ds_train_images = ds_train_images.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    halfBatch = int(batchSize / 2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for real_imgs in ds_train_images:\n",
    "            # Train Discriminator\n",
    "            idx = np.random.randint(0, real_imgs.shape[0], halfBatch)\n",
    "            real_half_batch = tf.gather(real_imgs, idx)\n",
    "\n",
    "            noise = np.random.normal(0, 1, (halfBatch, 100))\n",
    "            gen_imgs = generator.predict(noise)\n",
    "\n",
    "            d_loss_real = discriminator.train_on_batch(real_half_batch, tf.ones((halfBatch, 1)))\n",
    "            d_loss_fake = discriminator.train_on_batch(gen_imgs, tf.zeros((halfBatch, 1)))\n",
    "\n",
    "            d_loss = 0.5 * tf.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train Generator\n",
    "            noise = np.random.normal(0, 1, (batchSize, 100))\n",
    "            valid_y = np.ones((batchSize, 1))\n",
    "            g_loss = combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss}]\")\n",
    "\n",
    "        if epoch % saveInterval == 0:\n",
    "            save_imgs(epoch)\n",
    "\n",
    "# Save Images\n",
    "def save_imgs(epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(f\"images/mnist_{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Discriminator\n",
    "discriminator_model = discriminator()\n",
    "discriminator_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Generator\n",
    "generator_model = generator()\n",
    "generator_model.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "# Combined Model\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "discriminator_model.trainable = False\n",
    "valid = discriminator_model(img)\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "# Train GAN\n",
    "train(epochs=100, batchSize=32, saveInterval=10)\n",
    "\n",
    "# Save Generator Model\n",
    "generator.save('generator_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70857031-e3fe-4ef4-9aad-0fdedc0e7000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"asdasdasd\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
